# Inferencing LLM models locally

ONNX runtime works a as a runtime for LLM models, exposing endpoints locally to inference with the model.